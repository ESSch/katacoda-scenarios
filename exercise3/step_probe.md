# Оптимизацию образа и контейнера
## Предисловие
В этом уроке мы рассмотрим особенности работы тяжёлых приложений в облаке.
## Предистория
Для примера мы возьмём: `curl -L -s -o /root/exercise/cifar.py https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py`{{execute T1}}. Более подробно Вы можете узнать о самой программе из документации и нашего курса https://sberbank-school.ru/programs/8538/about . Мы его берём не случайно, ведь ML лидирует по потреблению ресурсов в SberCloud, а количество сотрудников возрастёт в 4 раза.

Аналитик передаёт архитектору оценочный чеклист стандарта CloudNative, в котором заявлено **соответствие** приложения следующим требованиям стандарта:
> 1. 28 ЦИ-3 Запретить обращение к внешним сервисам для первоначального запуска контейнера - загружать все настройки, необходимые для запуска компонента, при старте контейнера из ConfigMap, Secrets, внешнего хранилища конфигураций, интегрированного в среду исполнения.
> 1. 29 ЦИ-4 Использовать контейнеры максимальным размером не более 1024 Мб
> 1. 30 ЦИ-5 Гарантировать максимальное время старта контейнера c загрузкой образа не более 30 секунд
> 1. 32 ЦИ-7 Использовать поды с максимумом 8 Гб памяти
> 1. 33 Ци-8 Использовать поды с максимумом 4 ядрами 
> 1. 22 RS-3.1 Обеспечивать горизонтальное масштабирование через создание stateless экземпляров компонент средствами оркестратора контейнеров

## Поднимите окружение
Запустим контейнер:
``date && docker run -it --rm --name pytorch pytorch/pytorch bash -c "date && pip install ipywidgets matplotlib > /dev/null && apt update -y && apt install -y wget && wget https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py && date && python3 cifar10_tutorial.py && date"``{{execute T1}}

## Анализ нарушений
ЦИ-3. Мы видим, что контейнер при старте устанавливает библиотеки (wget), скачивает скрипт, да и сам dataset скачивается внутри скрипта.

ЦИ-4. Посмотрим на размер образа `docker images | grep pyto`{{execute T1}} - он 3,47Gb, а по стандарту он должен быть не более 1Gb.

ЦИ-5. Время начала скачивания образа у меня Mon Oct 26 07:30:03 UTC 2020, а завершения - Mon Oct 26 07:31:57 UTC 2020, что по времени занимает 1:54. Время старта контейнера Mon Oct 26 07:35:10 UTC 2020, а завершение - Mon Oct 26 07:43:05 UTC 2020, тем самым подготовка заняла контейнера заняло 7:55, то есть старт приложения занял 9:49, а по стандарту должно быть не более 0:30.

## Уменьшение зависимостей и ускорение старта
Первое, что бросается в глаза, при взгляде на команду запуска, это установка wget, ipywidgets и matplotlib в нарушении ЦИ-3 - так как в продуктовой среде не будет доступа к внешним сервисам и все зависимости должны быть уже в образе. Второе - после скачивания реестра доступных пакетов с помощью `apt update` - отсутствие их очистки `apt-get clean all`. Третье - запуск в приложения в оболочке BASH, что не позволит работать с приложением напрямую и передать приложению сигнал на остановку при перемещении пода. Поручим разработчику разработать создание образа:
``
cat << 'EOF' > Dockerfile
FROM pytorch/pytorch AS dev
WORKDIR /workspace
ADD https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py /workspace
RUN pip install ipywidgets matplotlib
CMD ["/opt/conda/bin/python3", "/workspace/cifar10_tutorial.py"]
EOF
``{{execute T1}}
``
cat Dockerfile | sudo docker build -t cifar10_tutorial:0.1 -
docker images | grep cifar10_tutorial
sudo docker rmi cifar10_tutorial:0.1
``{{execute T1}}

Теперь образ может быть создан в pipeline и уже запущен без внешних зависимостей в установочных скриптах на тестовом стенде где-то за 1 секунду:
``
date && sudo docker run bash -c 'date && /opt/conda/bin/python3  cifar10_tutorial.py'
``{{execute T1}}
Но у нас остаётся обучение и скачивание cifar-10-python.tar.gz во время него.

## Уменьшение разамера образа - раздление образов по назначению

Посмотрим на слои образа `docker history pytorch/pytorch`{{execute T1}}. Мы видим, что самый большой слой conda (3.37GB) - это пакеты платформы conda. Мы можем посмотреть на более маленькие образы, в которых меньше пакетов, например, на `docker pull bitnami/pytorch`, но всё равно размер образа велик - 2.63GB (`docker images | grep bitnami/pytorch`). 

Проблема в отсутствии разделения окружений на окружении для разработки и сборки и окружения для запуска на производственной среде. Например:
* если мы разрабатываем на Go - для сборки нам нужны исходники библиотек, компилятор и программная оболочка, для запуска нам нужен только результирующий бинарный файл.
* если мы разрабатываем на Java нам нужны исходники проекта, оболочка и Maven/Gradle, а для выполнения Java-машина и JAR-архив. 
* если мы разрабатываем Front-end, то нем нужен оболочка, фреймворк с его cli, исходники проекта и библиотек, NodeJS c его библиотеками, а для выполнения - несколько текстовых файлов (css, js, html). 

В нашем примере для разработки и обучения нужна платформа Anaconda с pytorch, а для запуска - python. В случае с Go нам нужно получить бинарный файл и скопировать в новое окружение, в случае с Java - JAR-архив, в случае с ML - обученная модель. Мы видели, что результатом обучения являлся файл `./cifar_net.pth`, который использовался в тестировании. Попросим: 
* разработчика cделать два скрипта: для обучения и часть для тестирования 
* DevOps инженера сделать скрипт (`Dockerfile_multi`) двумя образами: для создания и тестирования
``
cd /root/exercise/ && sudo docker build -f Dockerfile_multi -t cifar10_tutorial:0.2 .
``{{execute T1}}
``
docker images | grep cifar10_tutorial
sudo docker rmi cifar10_tutorial:0.2
``{{execute T1}}

sudo docker run --rm --name cifar cifar10_tutorial:0.2 ls /tmp


sudo docker images | grep cifar10_tutorial
sudo docker run --name cifar -it cifar10_tutorial:0.2 bash 
cd /root/exercise/ && sudo docker build -f Dockerfile_small -t cifar10_cuda .  

## Уменьшение размера образа - использование минимального базового образа для продуктовой среды

В отличии от разработки, где нам требуется определённое окружение и инструментарий, для продуктовой среды используется минимальный набор, необходимый для запуска приложения. В нашем случае нам нужен сервер, способный выдавать предсказания на основе входных данных на основе уже обученной моделей. Для этого уже есть готовый образ с минимальным размером и встроенным сервером, для отдачи результата пользователям.

Можно написать Web-сервер на Python который будет принимать изображения, а Pytorh трансформировать их в тензеры (transforms.Compose) и проверять. Кроме медлительности размер образа будет больше 3.37GB.

Pytorh предоставляет готовый сервер Torchserve для прогнозирования. Torchserve содержит в себе и python и различные утилиты torch-model-archiver и при этом весит 1.92GB: 
``
docker pull pytorch/torchserve 
docker images | grep torchserve
docker run -it --rm --name torchserve pytorch/torchserve python --version
docker run -it --rm --name torchserve pytorch/torchserve torch-model-archiver -v
``{{execute T1}}

tensorflow/serving не содержит в себе лишнего и весит только 0.286GB (нужно пробросить модель через ONNX):
``
docker pull tensorflow/serving
docker images | grep serving
``{{execute T1}}

Проверим его работу:
``
git clone https://github.com/tensorflow/serving
TESTDATA="$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata"
docker run -t --rm -p 8501:8501 \
    -v "$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two" \
    -e MODEL_NAME=half_plus_two \
    tensorflow/serving &
curl -d '{"instances": [1.0, 2.0, 5.0]}' \
    -X POST http://localhost:8501/v1/models/half_plus_two:predict
``{{execute T1}}

## Объём памяти и скорость работы.
Для большей высокой и стабильной точности разработчики по аналогии с GoogLeNet решили сделать ансамбль нескольких сетей, чтобы усреднить их предсказания. Четыре конфигурации сети, запуская их процессами. Из-за того, что PyTorch потребляет примерно 2Gb на ядро Cuda - то это приведёт к потреблению более 8Gb (нарушению Ци-7) и более 4 ядер (4 ядра на воркеры и одно на родительский процесс) (нарушает Ци-8). 

## Горизонтальное масштабирование
Далее они посоветовались с Data-аналитиком и переписали самодельное ансамблирование на специализированную библиотеку Sklearn. И в коде выглядит красива - небольшая обёртка перед моделью в скрипте, а по факту библиотека делает копии моделей и запускает обучение несколько моделей в одном контейнере. Дублирование данных сродни дублированию кода. Такой подход масштабирования приводит к множественным процессам и дублированию данных, а масштабирование, пусть и скрытое в коде, противоречит требованию осуществлять масштабирование средствами оркестратора (RS-3.1). Правильной конфигурацией будет управляющий сервер и воркеры. В таком случае лучше посмотреть в сторону готовых систем Kubeflow и MLflow.