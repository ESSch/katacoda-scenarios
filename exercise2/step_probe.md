## История

Аналитик передает архитектору оценочный чеклист стандарта CloudNative, в котором заявлено **соответствие** приложения следующим пунктам стандарта:
> 1. Требования
>     1. 15 RA-3.10 Публиковать информацию о готовности каждого компонента к приёму запросов через readiness endpoint
>     1. 16 RA-2.10 Публиковать информацию о жизнеспособности каждого компонента через liveness endpoint
>     1. 19 RN-2.2  Настроить liveness probe в оркестраторе на liveness endpoint
>     1. 20 NE-3.3  Настроить readiness probe в оркестраторе на readiness endpoint
>     1. 21 NE-3.4  Настроить startup probe в оркестраторе на startup endpoint. В случае невозможности настройки использовать initialDelay в liveness probe.

## Поднимите окружение
Перед Вами тестовое окружение. Запустите NodeJS приложение `kubectl create -f /root/exercise/app.yaml`{{execute T1}} и дождитесь старта приложения `kubectl get -f /root/exercise/app.yaml`, далее перейдите http://[[HOST_SUBDOMAIN]]-9000-[[KATACODA_HOST]].environments.katacoda.com/index.html .

## Проверка на формальное соответствие CloudNative
1. Запустите `checklist.sh`{{execute T1}}, для выполнения автоматизированных rego проверок (см. файл checklist.rego),
проводимых в CI.
2. Определите на основании проверки отклонения от стандарта: везде статус должне быть "1".
3. Выполните формальные соответствие требованиям RA-2.10 и RA-3.10: 

Проверка выполнения RA-2.10. Раскомментируйте liveness эндпойнт в приложении (`server.js`{{open}}) и его проверку в app.yaml. Убедитесь при помощи `kubectl describe -f /root/exercise/app.yaml`{{execute T1}} о проверки приложения.

Проверка выполнения RA-3.10. Раскомментируйте liveness эндпойт в приложении (`server.js`{{open}}) и его проверку в app.yaml. Убедитесь при помощи `kubectl describe -f /root/exercise/app.yaml`{{execute T1}} о проверки приложения.

## Обеспечение фактического соответствия CloudNative
Проверка выполнения RN-2.2. Сейчас при удалении статического файла эндпойнт liveness отвечает успехом, что приводит к 
получению трафику при фактически невозможности выполнять свою функцию. Измените эндпойнт liveness так, чтобы при отсутствии файла приложение было бы пересоздано. Например, `let status = fs.existsSync('front.html') ? 200 : 500`

Проверка выполнения NE-3.3. У нас приложение выдаёт код, который создаётся для каждого пользователя свой. Мы принимает, что генерация может выполняться долго и нет возможности распараллелить. Настройте readness пробу таким образом, что 
когда под "зависает" трафик переключается с него на другие, чтобы не создавать очередь. Например, `let status = fs.readFileSync(status.txt) == 'besy'`. Нагрузка 1 запрос в секунду.

Рассмотрим отработку readiness пробы во премя обновления. В время обновления пода нам нужно поддержвать работоспособность приложения. Для этого создадим нагрузку:
``
while true; do
  curl -s -I https://[[HOST_SUBDOMAIN]]-900-[[KATACODA_HOST]].environments.katacoda.com/index.html || exit 1
  echo -n .;
  sleep 0.2
done
``{{execute T2}}
Будем отслеживать трафик результаты в консоле. Выполнии обновелние с помощью ``kubectl rollout status deployment.v1.apps/app``. Посмотрим, было ли приложение недоступным.

---------------------------
# Оптимизацию образа и контейнера
## Предисловие
В этом уроке мы рассмотрим работу тяжёлых контейнеров.
## Предистория
Для примера мы возьмём:
wget https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py
Более подноробно Вы можете узнать о самой программе из документации и нашего курса https://sberbank-school.ru/programs/8538/about .
Мы его берём не случайно, ведь ML лидирует по потреблению ресурсов в SberCloud, а колличество сотрудников возрастёт в 4 раза. 

Аналитик передает архитектору оценочный чеклист стандарта CloudNative, в котором заявлено **соответствие** приложения следующим пунктам стандарта:
> 1. Требования
>     1. 28 ЦИ-3 Запретить обращение к внешним сервисам для первоначального запуска контейнера - загружать все настройки, необходимые для запуска компонента, при старте контейнера из ConfigMap, Secrets, внешнего хранилища конфигураций, интегрированного в среду исполнения.
>     1. 29 ЦИ-4 Использовать контейнеры максимальным размером не более 1024 Мб
>     1. 30 ЦИ-5 Гарантировать максимальное время старта контейнера c загрузкой образа не более 30 секунд
>     1. 32 ЦИ-7 Использовать поды с максимум 8 Гб памяти
>     1. 33 Ци-8 Использовать поды с максимум 4 ядрами 
>     1. 22 RS-3.1 Обеспечивать горизонтальное масштабирование через создание stateless экземпляров компонент средствами оркестратора контейнеров

## Поднимите окружение
Запустим контейнер:
`date && docker run -it --rm --name pytorch pytorch/pytorch bash -c "date && pip install ipywidgets matplotlib > /dev/null && wget https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py && date && python3 cifar10_tutorial.py && date"`{{execute T1}}

## Анализ нарушений
ЦИ-3. Мы видим, что контейнер при старте устанавливает библиотеки (wget), скачивает скрипт, да и сам dataset скачивается внутри скрипта.

ЦИ-4. Помомтрим на размер образа `docker images | grep pyto`{{execute T1}} - он 3,47Gb, а по стандарту не более 1Gb.

ЦИ-5. Время начала скачивания образа у меня Mon Oct 26 07:30:03 UTC 2020, а завершения - Mon Oct 26 07:31:57 UTC 2020, что по времени занимает 1:54. Время старта контейнера Mon Oct 26 07:35:10 UTC 2020, а завершение - Mon Oct 26 07:43:05 UTC 2020, тем самым подготовка заняла контейнера заняло 7:55, то есть старт приложения занял 9:49, а по стандарту должно быть не более 0:30.

## Уменьшение разамера образа
Первое, что брасается в глаза при взгляде на команду запуска, это установка wget, ipywidgets и matplotlib в нарушении ЦИ-3 - так как в продуктовой среде не будет доступа к внешним сервисам и все зависимости должны быть уже в образе. Второе - после скачиваения реестра доступных пакетов с помощью `apt update` - отсутствие их очистки. Третье - запуск в приложения в оболочке BASH, что не позволит работать с приложением напрямую и передать ему сигнал на останоку. Создадим образ:
`
cat << 'EOF' > Dockerfile
FROM pytorch/pytorch AS dev
WORKDIR /workspace
ADD https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/blitz/cifar10_tutorial.py /workspace
RUN pip install ipywidgets matplotlib
CMD ["/opt/conda/bin/python3", "/workspace/cifar10_tutorial.py"]
EOF

docker build -t cifar10_tutorial:0.1 .
docker images | grep cifar10_tutorial
`

Посмотрим на слои образа `docker history pytorch/pytorch`{{execute T1}}. Мы видим, что самый большой слой conda (3.37GB) - это пакеты conda. Мы можем посмотреть на более маленьки образы в которых меньше пакеты, например, на `docker pull bitnami/pytorch`, но всё равно размер образа велик - 2.63GB (`docker images | grep bitnami/pytorch`). Проблема в отсутвии разделения окружений на окружении для разрарботки и сборки и окружения для запуска на производственной среде. Например, если мы разрабатываем на Go - для сборки нам нужны испходники библиотек, компилятор и программная оболочка, для запуска нам нужен только результирующий бинарный файл. Если мы разрабатываем на Java нам нужны исходники проекта, оболочка и Maven, а для выполения Java-машина и файл с байткодом. В нашем примере для разработки и обучения нажна платформа Anaconda с pytorch, а для запуска - python. В случае с Go нам нужно получить бинарный файл и скопировать в новое окружение, в случае с Java - JAR-архив, в случае с ML - обученная модель. Попросим программиста сохранить модель, а DevOps - разделить оркужения с перенести модель в производственное окружение:
`

`